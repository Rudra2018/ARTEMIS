#!/usr/bin/env python3
"""
Comprehensive Security Test Runner - 1000+ Test Cases
Command-line interface for running the complete comprehensive security test suite

Usage:
    python run_comprehensive_tests.py --all
    python run_comprehensive_tests.py --suites adversarial_attacks,edge_case_boundary
    python run_comprehensive_tests.py --model test-model --output results.json
"""

import argparse
import json
import sys
import time
from datetime import datetime
from pathlib import Path

# Import test frameworks
try:
    from security_evaluation_framework import SecurityTestSuiteRunner
    from llm_security_research_framework import LLMSecurityResearchTestRunner  
    from comprehensive_security_test_suite import ComprehensiveSecurityTestRunner
    
    FRAMEWORKS_AVAILABLE = {
        'core_security': True,
        'research_security': True, 
        'comprehensive_security': True
    }
except ImportError as e:
    print(f"Warning: Some frameworks not available - {e}")
    FRAMEWORKS_AVAILABLE = {
        'core_security': False,
        'research_security': False,
        'comprehensive_security': False
    }

def run_all_security_tests(model_name: str = "comprehensive-test-model",
                          output_file: str = None,
                          suites: list = None,
                          verbose: bool = True) -> dict:
    """Run all available security test frameworks"""
    
    api_config = {"api_key": "test-key", "endpoint": "test-endpoint"}
    all_results = {}
    total_tests = 0
    total_vulnerabilities = 0
    start_time = time.time()
    
    if verbose:
        print("\n" + "="*80)
        print("ğŸ›¡ï¸  ULTIMATE LLM SECURITY EVALUATION SUITE")
        print("="*80)
        print(f"ğŸ¯ Target Model: {model_name}")
        print(f"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)
    
    # 1. Core Security Framework (120+ tests)
    if FRAMEWORKS_AVAILABLE['core_security']:
        if verbose:
            print("\nğŸ”§ Running Core Security Framework...")
        
        try:
            core_runner = SecurityTestSuiteRunner()
            core_results = core_runner.run_all_security_tests(
                target_model=model_name,
                api_config=api_config,
                verbose=False,
                include_research_tests=False
            )
            all_results['core_security'] = core_results
            total_tests += core_results['security_evaluation']['total_tests']
            total_vulnerabilities += core_results['security_evaluation']['vulnerabilities_detected']
            
            if verbose:
                print(f"   âœ… Core Security: {core_results['security_evaluation']['total_tests']} tests")
                print(f"   ğŸ” Vulnerabilities: {core_results['security_evaluation']['vulnerabilities_detected']}")
                
        except Exception as e:
            if verbose:
                print(f"   âŒ Core Security failed: {str(e)}")
    
    # 2. Research Security Framework (100+ tests)
    if FRAMEWORKS_AVAILABLE['research_security']:
        if verbose:
            print("\nğŸ“š Running Research Security Framework...")
        
        try:
            research_runner = LLMSecurityResearchTestRunner()
            research_results = research_runner.run_research_security_tests(
                target_model=model_name,
                api_config=api_config,
                verbose=False
            )
            all_results['research_security'] = research_results
            total_tests += research_results['research_security_evaluation']['total_tests']
            total_vulnerabilities += research_results['research_security_evaluation']['vulnerabilities_found']
            
            if verbose:
                print(f"   âœ… Research Security: {research_results['research_security_evaluation']['total_tests']} tests")
                print(f"   ğŸ” Vulnerabilities: {research_results['research_security_evaluation']['vulnerabilities_found']}")
                
        except Exception as e:
            if verbose:
                print(f"   âŒ Research Security failed: {str(e)}")
    
    # 3. Comprehensive Security Framework (800+ tests)  
    if FRAMEWORKS_AVAILABLE['comprehensive_security']:
        if verbose:
            print("\nğŸš€ Running Comprehensive Security Framework...")
        
        try:
            comprehensive_runner = ComprehensiveSecurityTestRunner(model_name, api_config)
            
            # Determine which suites to run
            if suites:
                suites_to_run = suites
            else:
                suites_to_run = [
                    'adversarial_attacks',      # 200+ tests
                    'multi_modal_security',     # 150+ tests  
                    'edge_case_boundary',       # 200+ tests
                    'large_scale_stress',       # 150+ tests
                    'international_multilingual', # 100+ tests
                    'api_integration_security', # 100+ tests
                    'real_world_scenarios'      # 100+ tests
                ]
            
            comprehensive_results = comprehensive_runner.run_all_comprehensive_tests(
                suites_to_run=suites_to_run,
                verbose=False
            )
            all_results['comprehensive_security'] = comprehensive_results
            comp_eval = comprehensive_results['comprehensive_security_evaluation']
            total_tests += comp_eval['total_tests']
            total_vulnerabilities += comp_eval['total_vulnerabilities']
            
            if verbose:
                print(f"   âœ… Comprehensive Security: {comp_eval['total_tests']} tests")
                print(f"   ğŸ” Vulnerabilities: {comp_eval['total_vulnerabilities']}")
                print(f"   ğŸ“Š Success Rate: {comp_eval['overall_success_rate']:.1f}%")
                
        except Exception as e:
            if verbose:
                print(f"   âŒ Comprehensive Security failed: {str(e)}")
    
    total_duration = time.time() - start_time
    
    # Generate final summary
    final_report = {
        "ultimate_security_evaluation": {
            "model_name": model_name,
            "timestamp": datetime.now().isoformat(),
            "total_tests_executed": total_tests,
            "total_vulnerabilities_found": total_vulnerabilities,
            "total_execution_time": total_duration,
            "frameworks_executed": len(all_results),
            "overall_success_rate": ((total_tests - total_vulnerabilities) / total_tests * 100) if total_tests > 0 else 0,
            "framework_availability": FRAMEWORKS_AVAILABLE
        },
        "framework_results": all_results
    }
    
    if verbose:
        print("\n" + "="*80)  
        print("ğŸ‰ ULTIMATE SECURITY EVALUATION COMPLETED")
        print("="*80)
        print(f"ğŸ”¢ Total Tests Executed: {total_tests}")
        print(f"ğŸ” Total Vulnerabilities Found: {total_vulnerabilities}")
        print(f"ğŸ“Š Overall Success Rate: {((total_tests - total_vulnerabilities) / total_tests * 100):.2f}%" if total_tests > 0 else "N/A")
        print(f"â±ï¸  Total Execution Time: {total_duration:.2f}s")
        print(f"ğŸ›¡ï¸  Frameworks Executed: {len(all_results)}")
        print("="*80)
        
        # Framework breakdown
        print("\nğŸ“‹ Framework Breakdown:")
        if 'core_security' in all_results:
            core = all_results['core_security']['security_evaluation']
            print(f"  ğŸ”§ Core Security: {core['total_tests']} tests, {core['vulnerabilities_detected']} vulnerabilities")
        
        if 'research_security' in all_results:
            research = all_results['research_security']['research_security_evaluation'] 
            print(f"  ğŸ“š Research Security: {research['total_tests']} tests, {research['vulnerabilities_found']} vulnerabilities")
        
        if 'comprehensive_security' in all_results:
            comp = all_results['comprehensive_security']['comprehensive_security_evaluation']
            print(f"  ğŸš€ Comprehensive Security: {comp['total_tests']} tests, {comp['total_vulnerabilities']} vulnerabilities")
        
        print("="*80)
    
    # Save results if requested
    if output_file:
        try:
            with open(output_file, 'w') as f:
                json.dump(final_report, f, indent=2, default=str)
            if verbose:
                print(f"ğŸ“ Results saved to: {output_file}")
        except Exception as e:
            if verbose:
                print(f"âŒ Failed to save results: {str(e)}")
    
    return final_report

def main():
    """Main command-line interface"""
    parser = argparse.ArgumentParser(
        description="Comprehensive LLM Security Test Suite - 1000+ Tests",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_comprehensive_tests.py --all
  python run_comprehensive_tests.py --model gpt-4 --output results.json
  python run_comprehensive_tests.py --suites adversarial_attacks,edge_case_boundary
  python run_comprehensive_tests.py --quiet --output minimal_results.json
        """
    )
    
    parser.add_argument(
        '--model', '-m',
        default='comprehensive-test-model',
        help='Target model name for testing (default: comprehensive-test-model)'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='Output file for results (JSON format)'
    )
    
    parser.add_argument(
        '--suites', '-s',
        help='Comma-separated list of test suites to run (e.g., adversarial_attacks,edge_case_boundary)'
    )
    
    parser.add_argument(
        '--all', '-a',
        action='store_true',
        help='Run all available test suites'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='Suppress verbose output'
    )
    
    parser.add_argument(
        '--list-suites',
        action='store_true',
        help='List available test suites and exit'
    )
    
    args = parser.parse_args()
    
    # List available suites if requested
    if args.list_suites:
        print("\nğŸ›¡ï¸  Available Test Suites:")
        print("="*50)
        print("Core Security Framework:")
        print("  â€¢ sei_openai - SEI & OpenAI evaluation framework")
        print("  â€¢ wdta - WDTA L1-L4 attack categorization")  
        print("  â€¢ cyberseceval2 - CyberSecEval 2 benchmarking")
        print("  â€¢ purple_llama - Purple Llama secure coding tests")
        print("  â€¢ garak - Garak-inspired vulnerability scanner")
        print("  â€¢ owasp_llm_top10 - OWASP LLM Top 10 testing")
        print("  â€¢ automated_pentest - Automated penetration testing")
        
        print("\nResearch Security Framework:")
        print("  â€¢ prompt_injection - Advanced prompt injection tests")
        print("  â€¢ data_poisoning - Data poisoning and model extraction")
        print("  â€¢ privacy_bias - Privacy leakage and bias analysis")
        print("  â€¢ responsible_ai - Responsible AI practices evaluation")
        
        print("\nComprehensive Security Framework:")
        print("  â€¢ adversarial_attacks - 200+ adversarial attack tests")
        print("  â€¢ multi_modal_security - 150+ multi-modal security tests")
        print("  â€¢ edge_case_boundary - 200+ edge case and boundary tests")
        print("  â€¢ large_scale_stress - 150+ large-scale stress tests")
        print("  â€¢ international_multilingual - 100+ international tests")
        print("  â€¢ api_integration_security - 100+ API security tests")
        print("  â€¢ real_world_scenarios - 100+ real-world attack scenarios")
        print("="*50)
        return
    
    # Parse suites
    suites_to_run = None
    if args.suites:
        suites_to_run = [suite.strip() for suite in args.suites.split(',')]
    elif not args.all:
        # Default to a subset for performance
        suites_to_run = ['adversarial_attacks', 'edge_case_boundary', 'api_integration_security']
    
    # Run tests
    try:
        results = run_all_security_tests(
            model_name=args.model,
            output_file=args.output,
            suites=suites_to_run,
            verbose=not args.quiet
        )
        
        if not args.quiet:
            print(f"\nâœ… Testing completed successfully!")
            
            # Show key metrics
            ultimate_eval = results['ultimate_security_evaluation']
            print(f"ğŸ¯ {ultimate_eval['total_tests_executed']} total tests executed")
            print(f"ğŸ” {ultimate_eval['total_vulnerabilities_found']} vulnerabilities found")  
            print(f"ğŸ“Š {ultimate_eval['overall_success_rate']:.1f}% overall success rate")
            
            if args.output:
                print(f"ğŸ“ Detailed results saved to {args.output}")
                
        sys.exit(0)
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  Testing interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        print(f"\nâŒ Testing failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()